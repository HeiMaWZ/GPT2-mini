# GPT2-mini
This repository primarily aims to replicate GPT-2, including multi-head self-attention mechanisms, attention masking, data preprocessing, and inference. Ultimately, it achieves the effect of text generation
